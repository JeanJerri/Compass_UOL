{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba2cce92-9c43-4931-bb47-100db26619d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[659, 80, 892, 466, 380, 761, 830, 945, 180, 850, 544, 623, 635, 28, 192, 858, 233, 477, 602, 652, 358, 782, 349, 46, 242, 356, 813, 386, 363, 91, 543, 146, 303, 566, 361, 272, 914, 842, 344, 206, 829, 130, 864, 977, 143, 115, 421, 248, 644, 164, 363, 691, 436, 442, 151, 740, 171, 975, 967, 56, 966, 643, 534, 119, 370, 552, 159, 997, 339, 390, 327, 480, 624, 718, 637, 699, 107, 606, 769, 201, 617, 171, 535, 447, 387, 996, 514, 822, 781, 184, 776, 586, 838, 570, 56, 507, 228, 561, 572, 216, 714, 122, 428, 998, 113, 324, 108, 51, 251, 898, 846, 973, 435, 318, 961, 454, 703, 878, 399, 681, 434, 553, 209, 827, 702, 260, 411, 58, 462, 508, 957, 607, 689, 491, 747, 840, 457, 715, 281, 702, 31, 655, 454, 402, 434, 380, 952, 148, 515, 495, 617, 967, 598, 401, 172, 108, 459, 699, 560, 196, 862, 922, 829, 875, 825, 895, 48, 2, 106, 606, 761, 694, 826, 512, 659, 110, 463, 113, 304, 967, 14, 510, 296, 432, 406, 729, 967, 577, 802, 920, 261, 24, 853, 506, 518, 208, 326, 781, 741, 685, 12, 770, 807, 631, 155, 828, 964, 879, 729, 23, 264, 959, 263, 662, 529, 74, 623, 7, 887, 444, 553, 663, 932, 609, 898, 289, 390, 619, 430, 474, 215, 248, 980, 788, 564, 559, 765, 786, 336, 115, 388, 387, 122, 182, 67, 18, 460, 563, 32, 74]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import random\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "lista_aleatoria = [random.randint(0, 1000) for _ in range(250)]\n",
    "#print(lista_aleatoria)\n",
    "#print()\n",
    "lista_aleatoria.reverse()\n",
    "print(lista_aleatoria)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51418594-793f-45f4-9a6a-911e86c9a14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bode\n",
      "Boi\n",
      "Cabra\n",
      "Cachorro\n",
      "Capivara\n",
      "Cavalo\n",
      "Cobra\n",
      "Coelho\n",
      "Crocodilo\n",
      "Elefefante\n",
      "Gato\n",
      "Girafa\n",
      "Guepardo\n",
      "Hipopotamo\n",
      "Jacaré\n",
      "Leão\n",
      "Peixe\n",
      "Rato\n",
      "Tatu\n",
      "Vaca\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "animais = [\"Vaca\", \"Cachorro\", \"Gato\", \"Peixe\", \"Cavalo\", \"Cobra\", \n",
    "           \"Coelho\", \"Rato\", \"Girafa\", \"Leão\", \"Jacaré\", \"Bode\",\n",
    "          \"Cabra\", \"Tatu\", \"Boi\", \"Elefefante\", \"Hipopotamo\",\n",
    "          \"Capivara\", \"Crocodilo\", \"Guepardo\"]\n",
    "animais.sort()\n",
    "#print(animais)\n",
    "#print()\n",
    "\n",
    "[print(animal) for animal in animais]\n",
    "\n",
    "with open(\"animais.csv\", \"w\") as arquivo:\n",
    "    for animal in animais:\n",
    "        arquivo.write(animal + \"\\n\")\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d4094de-5691-49ee-9d7c-f13cf45aff18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting names\n",
      "  Downloading names-0.3.0.tar.gz (789 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m789.1/789.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: names\n",
      "  Building wheel for names (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for names: filename=names-0.3.0-py3-none-any.whl size=803681 sha256=fc2328a1850d2a7dc99580f222877801409a6e47cb71fb6abdbd300e604763f5\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/8d/db/fc/50ec19a89a8dcbbd158a4aae44123cb525cda1f07dae287197\n",
      "Successfully built names\n",
      "Installing collected packages: names\n",
      "Successfully installed names-0.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6805837-88ae-48a2-a72a-c02ffb79b5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gerando 10000000 nomes aleatórios\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import names\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "random.seed(40)\n",
    "qtd_nomes_unicos = 3000\n",
    "qtd_nomes_aleatorios = 10000000\n",
    "\n",
    "aux=[]\n",
    "for i in range(0, qtd_nomes_unicos):\n",
    "   aux.append(names.get_full_name())\n",
    "    \n",
    "print(\"Gerando {} nomes aleatórios\".format(qtd_nomes_aleatorios))\n",
    "\n",
    "dados=[]\n",
    "for i in range(0,qtd_nomes_aleatorios):\n",
    "   dados.append(random.choice(aux))\n",
    "\n",
    "with open(\"nomes_aleatorios.txt\", \"w\") as arquivo:\n",
    "    for nomes in dados:\n",
    "        arquivo.write(nomes + \"\\n\")\n",
    "\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
